{"version":3,"sources":["App.js","reportWebVitals.js","index.js"],"names":["App","useSpeechRecognition","transcript","finalTranscript","listening","resetTranscript","browserSupportsSpeechRecognition","useState","setHavePermissions","React","useEffect","speak","SpeechSynthesisUtterance","speechSynthesis","console","log","Mobile","onClick","navigator","mediaDevices","getUserMedia","audio","video","then","stream","alert","x","catch","err","name","message","recognition","window","speechRecognition","webkitSpeechRecognition","start","onresult","e","current","resultIndex","results","onCopy","preventDefault","SpeechRecognition","stopListening","startListening","continuous","reportWebVitals","onPerfEntry","Function","getCLS","getFID","getFCP","getLCP","getTTFB","ReactDOM","render","StrictMode","document","getElementById"],"mappings":"qOA6FeA,MAzFf,WACE,MAMIC,iCALFC,EADF,EACEA,WACAC,EAFF,EAEEA,gBACAC,EAHF,EAGEA,UACAC,EAJF,EAIEA,gBACAC,EALF,EAKEA,iCAEF,EAA8CC,oBAAS,GAAvD,mBAAwBC,GAAxB,WAcA,GAbAC,IAAMC,WAAU,WACd,GAAIP,EAAgB,CAClB,IAAMQ,EAAQ,IAAIC,yBAAyBT,GAC3CU,gBAAgBF,MAAMA,GACtBG,QAAQC,IAAIZ,MAEd,CAACA,KAOEG,EAEH,OADAQ,QAAQC,IAAR,+CACO,+EAET,SAASC,IA+BP,OAAO,wBAAQC,QAjBf,WAXsBC,UAAUC,aAAaC,aAAa,CAACC,OAAO,EAAMC,OAAO,IACjEC,MAAK,SAACC,GAChBC,MAAM,4BACNjB,GAAoB,SAAAkB,GAAC,OAAKA,QAE3BC,OAAM,SAACC,GACNpB,GAAmB,GACnBM,QAAQC,IAAR,UAAea,EAAIC,KAAnB,cAA6BD,EAAIE,aAMnC,IACIC,EAAc,IADQC,OAAOC,mBAAqBD,OAAOE,yBAE7DH,EAAYI,QACZJ,EAAYK,SAAW,SAACC,GACtB,IAAMC,EAAUD,EAAEE,YACdrC,EAAamC,EAAEG,QAAQF,GAAS,GAAGpC,WACL,GAAXoC,GAAgBpC,GAAcmC,EAAEG,QAAQ,GAAG,GAAGtC,YAG/C,mCAAfA,GAAyC,oCAAfA,GACzBY,QAAQC,IAAIb,KAKjB,0GAIT,OACE,gCACE,mFACAE,EACE,2BACA,8BAEF,yBACEqC,OAAQ,SAACJ,GAEP,OADAA,EAAEK,kBACK,GAETzB,QACE,kBAAMb,EACJuC,IAAkBC,iBAzD1BvC,SACAsC,IAAkBE,eAAe,CAAEC,YAAY,MAiD7C,kDAWA1C,EACE,iCACD,oCAED,4BAAIF,IACJ,cAACc,EAAD,QC5ES+B,EAZS,SAAAC,GAClBA,GAAeA,aAAuBC,UACxC,6BAAqB1B,MAAK,YAAkD,IAA/C2B,EAA8C,EAA9CA,OAAQC,EAAsC,EAAtCA,OAAQC,EAA8B,EAA9BA,OAAQC,EAAsB,EAAtBA,OAAQC,EAAc,EAAdA,QAC3DJ,EAAOF,GACPG,EAAOH,GACPI,EAAOJ,GACPK,EAAOL,GACPM,EAAQN,OCDdO,IAASC,OACP,cAAC,IAAMC,WAAP,UACE,cAAC,EAAD,MAEFC,SAASC,eAAe,SAM1BZ,M","file":"static/js/main.c7fcbad0.chunk.js","sourcesContent":["import React, { useState } from 'react';\nimport './App.css';\nimport SpeechRecognition, {useSpeechRecognition} from 'react-speech-recognition';\n\nfunction App() {\n  const {\n    transcript,\n    finalTranscript,\n    listening,  \n    resetTranscript,\n    browserSupportsSpeechRecognition\n  } = useSpeechRecognition();\n  const [havePermissions, setHavePermissions] = useState(false)\n  React.useEffect(() => {\n    if (finalTranscript){\n      const speak = new SpeechSynthesisUtterance(finalTranscript)\n      speechSynthesis.speak(speak)\n      console.log(finalTranscript);\n    }\n  },[finalTranscript])\n\n  const startListening = () => {\n    resetTranscript()\n    SpeechRecognition.startListening({ continuous: true });\n  }\n\n  if (!browserSupportsSpeechRecognition) {\n    console.log(`Browser doesn't support speech recognition.`);\n    return <span>Browser doesn't support speech recognition.</span>;\n  }\n  function Mobile(){\n    \n    function checkPermissions() {\n      const permissions = navigator.mediaDevices.getUserMedia({audio: true, video: false})\n      permissions.then((stream) => {\n        alert('accepted the permissions');\n        setHavePermissions( x => !x)\n      })\n      .catch((err) => {\n        setHavePermissions(false)\n        console.log(`${err.name} : ${err.message}`)\n      });\n    }\n    \n    function handleReco(){\n      checkPermissions()\n      const SpeechRecognition = window.speechRecognition || window.webkitSpeechRecognition;\n      let recognition = new SpeechRecognition()\n      recognition.start();\n      recognition.onresult = (e) => {\n        const current = e.resultIndex;\n        let transcript = e.results[current][0].transcript;\n        let mobileRepeatBug = (current == 1 && transcript == e.results[0][0].transcript);\n\n        if(!mobileRepeatBug) {\n            if(transcript === 'בדיקה' || transcript === ' בדיקה') {\n                console.log(transcript);\n            }\n        }\n    }\n    }\n    return <button onClick={handleReco}>\n      זיהוי קולי בטלפון\n    </button>\n  }\n  return (\n    <div>\n      <p>מיקרופון: {\n      listening \n      ? 'פעיל' \n      : 'כבוי'\n      }</p>\n      <button\n        onCopy={(e)=>{\n          e.preventDefault()\n          return false;\n        }}\n        onClick={\n          () => listening \n          ? SpeechRecognition.stopListening() \n          : startListening()\n        }\n      >לחיצה ל{\n      listening \n      ? 'דיבור' \n      :'כיבוי'\n      }</button>\n      <p>{transcript}</p>\n      <Mobile/>\n    </div>\n  );\n}\n\nexport default App;\n","const reportWebVitals = onPerfEntry => {\n  if (onPerfEntry && onPerfEntry instanceof Function) {\n    import('web-vitals').then(({ getCLS, getFID, getFCP, getLCP, getTTFB }) => {\n      getCLS(onPerfEntry);\n      getFID(onPerfEntry);\n      getFCP(onPerfEntry);\n      getLCP(onPerfEntry);\n      getTTFB(onPerfEntry);\n    });\n  }\n};\n\nexport default reportWebVitals;\n","import React from 'react';\nimport ReactDOM from 'react-dom';\nimport './index.css';\nimport App from './App';\nimport reportWebVitals from './reportWebVitals';\n\nReactDOM.render(\n  <React.StrictMode>\n    <App />\n  </React.StrictMode>,\n  document.getElementById('root')\n);\n\n// If you want to start measuring performance in your app, pass a function\n// to log results (for example: reportWebVitals(console.log))\n// or send to an analytics endpoint. Learn more: https://bit.ly/CRA-vitals\nreportWebVitals();\n"],"sourceRoot":""}