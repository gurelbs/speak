{"version":3,"sources":["logo.svg","App.js","reportWebVitals.js","index.js"],"names":["App","useSpeechRecognition","transcript","finalTranscript","listening","resetTranscript","browserSupportsSpeechRecognition","React","useEffect","speak","SpeechSynthesisUtterance","speechSynthesis","console","log","startListening","SpeechRecognition","continuous","onTouchStart","onMouseDown","onTouchEnd","stopListening","onMouseUp","reportWebVitals","onPerfEntry","Function","then","getCLS","getFID","getFCP","getLCP","getTTFB","ReactDOM","render","StrictMode","document","getElementById"],"mappings":"uJAAe,I,mCC6CAA,MAxCf,WAEE,MAMIC,iCALFC,EADF,EACEA,WACAC,EAFF,EAEEA,gBACAC,EAHF,EAGEA,UACAC,EAJF,EAIEA,gBACAC,EALF,EAKEA,iCAEFC,IAAMC,WAAU,WACd,GAAIL,EAAgB,CAClB,IAAMM,EAAQ,IAAIC,yBAAyBP,GAC3CQ,gBAAgBF,MAAMA,GACtBG,QAAQC,IAAIV,MAEd,CAACA,IACH,IAAMW,EAAiB,WACrBT,IACAU,IAAkBD,eAAe,CAAEE,YAAY,KAGjD,OAAKV,EAMH,gCACE,6CAAgBF,EAAY,KAAO,SACnC,wBACEa,aAAcH,EACdI,YAAaJ,EACbK,WAAYJ,IAAkBK,cAC9BC,UAAWN,IAAkBK,cAJ/B,0BAMA,4BAAIlB,OAbC,gFCfIoB,EAZS,SAAAC,GAClBA,GAAeA,aAAuBC,UACxC,8BAAqBC,MAAK,YAAkD,IAA/CC,EAA8C,EAA9CA,OAAQC,EAAsC,EAAtCA,OAAQC,EAA8B,EAA9BA,OAAQC,EAAsB,EAAtBA,OAAQC,EAAc,EAAdA,QAC3DJ,EAAOH,GACPI,EAAOJ,GACPK,EAAOL,GACPM,EAAON,GACPO,EAAQP,OCDdQ,IAASC,OACP,cAAC,IAAMC,WAAP,UACE,cAAC,EAAD,MAEFC,SAASC,eAAe,SAM1Bb,K","file":"static/js/main.078fac83.chunk.js","sourcesContent":["export default __webpack_public_path__ + \"static/media/logo.6ce24c58.svg\";","import logo from './logo.svg';\nimport './App.css';\nimport { createSpeechlySpeechRecognition } from '@speechly/speech-recognition-polyfill';\nimport SpeechRecognition, {useSpeechRecognition} from 'react-speech-recognition'\nimport React from 'react'\nfunction App() {\n\n  const {\n    transcript,\n    finalTranscript,\n    listening,  \n    resetTranscript,\n    browserSupportsSpeechRecognition\n  } = useSpeechRecognition();\n  React.useEffect(() => {\n    if (finalTranscript){\n      const speak = new SpeechSynthesisUtterance(finalTranscript)\n      speechSynthesis.speak(speak)\n      console.log(finalTranscript);\n    }\n  },[finalTranscript])\n  const startListening = () => {\n    resetTranscript()\n    SpeechRecognition.startListening({ continuous: true });\n  }\n\n  if (!browserSupportsSpeechRecognition) {\n    return <span>Browser doesn't support speech recognition.</span>;\n  }\n\n\n  return (\n    <div>\n      <p>Microphone: {listening ? 'on' : 'off'}</p>\n      <button\n        onTouchStart={startListening}\n        onMouseDown={startListening}\n        onTouchEnd={SpeechRecognition.stopListening}\n        onMouseUp={SpeechRecognition.stopListening}\n      >Hold to talk</button>\n      <p>{transcript}</p>\n    </div>\n  );\n}\n\nexport default App;\n","const reportWebVitals = onPerfEntry => {\n  if (onPerfEntry && onPerfEntry instanceof Function) {\n    import('web-vitals').then(({ getCLS, getFID, getFCP, getLCP, getTTFB }) => {\n      getCLS(onPerfEntry);\n      getFID(onPerfEntry);\n      getFCP(onPerfEntry);\n      getLCP(onPerfEntry);\n      getTTFB(onPerfEntry);\n    });\n  }\n};\n\nexport default reportWebVitals;\n","import React from 'react';\nimport ReactDOM from 'react-dom';\nimport './index.css';\nimport App from './App';\nimport reportWebVitals from './reportWebVitals';\n\nReactDOM.render(\n  <React.StrictMode>\n    <App />\n  </React.StrictMode>,\n  document.getElementById('root')\n);\n\n// If you want to start measuring performance in your app, pass a function\n// to log results (for example: reportWebVitals(console.log))\n// or send to an analytics endpoint. Learn more: https://bit.ly/CRA-vitals\nreportWebVitals();\n"],"sourceRoot":""}